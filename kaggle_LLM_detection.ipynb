{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#using libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "metadata": {
        "id": "ADQDXHRJxTpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "training_data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/KAGGLE_llm-detection/train_essays.csv\")\n",
        "testing_data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/KAGGLE_llm-detection/test_essays.csv\")\n",
        "\n",
        "# Initial data exploration\n",
        "training_data.describe()"
      ],
      "metadata": {
        "id": "m14Ce_CPy2QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function for text cleaning\n",
        "def preprocess_text(text):\n",
        "    # Removing punctuation and converting to lowercase\n",
        "    text = re.sub(r'\\W', ' ', text).lower()\n",
        "    # Tokenization and removal of stopwords\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word.isalpha() and word not in set(stopwords.words('english'))]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Apply text preprocessing\n",
        "training_data['processed_text'] = training_data['text'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "jHJdD8-zy2Nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data for training and validation\n",
        "X_train, X_validate, y_train, y_validate = train_test_split(training_data['processed_text'], training_data['generated'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Setup BERT tokenizer\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "ftB7ThzGy2K5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RGWD5HhWy2IE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_texts(tokenizer, texts):\n",
        "    return tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Encoding text data\n",
        "train_encodings = encode_texts(bert_tokenizer, X_train.tolist())\n",
        "validate_encodings = encode_texts(bert_tokenizer, X_validate.tolist())"
      ],
      "metadata": {
        "id": "9x-CxrPOy2FM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXM-y-nr0RWj",
        "outputId": "9eafa95a-659d-4c49-ebc4-46686c962f9f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare datasets for PyTorch\n",
        "def create_dataset(encodings, labels):\n",
        "    return TensorDataset(encodings['input_ids'], encodings['attention_mask'], torch.tensor(labels.values))\n",
        "\n",
        "train_dataset = create_dataset(train_encodings, y_train)\n",
        "validate_dataset = create_dataset(validate_encodings, y_validate)"
      ],
      "metadata": {
        "id": "qbfg17WPzFKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader setup\n",
        "batch_size = 16\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "validate_dataloader = DataLoader(validate_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "aOxaPVhizFHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize BERT model\n",
        "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "bert_model.to(device)\n",
        "\n",
        "# Optimizer configuration\n",
        "optimizer = AdamW(bert_model.parameters(), lr=2e-5)"
      ],
      "metadata": {
        "id": "twMq7eZ8y2CZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train_model(model, dataloader, optimizer, epochs=10):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch in dataloader:\n",
        "            batch = [item.to(device) for item in batch]\n",
        "            inputs, masks, labels = batch\n",
        "\n",
        "            model.zero_grad()\n",
        "            outputs = model(inputs, attention_mask=masks, labels=labels)\n",
        "            loss =outputs.loss\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Avoid exploding gradients\n",
        "            optimizer.step()\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {total_loss/len(dataloader):.4f}\")"
      ],
      "metadata": {
        "id": "lfvIDovqzOoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(bert_model, train_dataloader, optimizer)"
      ],
      "metadata": {
        "id": "05SyXUvjzOlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataloader):\n",
        "  model.eval()\n",
        "  predictions, true_labels = [], []\n",
        "  with torch.no_grad():\n",
        "    for batch in dataloader:\n",
        "      batch = [item.to(device) for item in batch]\n",
        "      inputs, masks, labels = batch\n",
        "      outputs = model(inputs, attention_mask=masks)\n",
        "      logits = outputs.logits\n",
        "\n",
        "    predictions.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "    true_labels.extend(labels.cpu().numpy())\n",
        "  accuracy = accuracy_score(true_labels, predictions)\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "rcK-0p9szOh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_accuracy = evaluate_model(bert_model, validate_dataloader)\n",
        "print(f\"Validation Accuracy: {validation_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "jqxwZYKbzOfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_encodings = encode_texts(bert_tokenizer, testing_data['text'].tolist())\n",
        "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'])\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "v02ZTh5my2AN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_test_data(model, dataloader):\n",
        "  model.eval()\n",
        "  test_predictions = []\n",
        "  with torch.no_grad():\n",
        "    for batch in dataloader:\n",
        "      inputs, masks = batch\n",
        "      inputs, masks = inputs.to(device), masks.to(device)\n",
        "      outputs = model(inputs, attention_mask=masks)\n",
        "      logits = outputs.logits\n",
        "      test_predictions.extend(torch.softmax(logits, dim=1)[:, 1].cpu().numpy())\n",
        "  return test_predictions\n",
        "\n",
        "test_predictions = predict_test_data(bert_model, test_dataloader)"
      ],
      "metadata": {
        "id": "guQBdM5Hz2-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_df = pd.DataFrame({'id': testing_data['id'],'generated': test_predictions})"
      ],
      "metadata": {
        "id": "bvcSvYSoz27s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_df.to_csv('/content/drive/MyDrive/Colab Notebooks/KAGGLE_llm-detection/submission.csv', index=False)"
      ],
      "metadata": {
        "id": "AYgtruxrz24Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lahhLmlmz21I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}